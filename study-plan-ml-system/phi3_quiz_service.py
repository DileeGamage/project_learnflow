"""
Phi-3-Mini Quiz Generation Service - Optimized for Chunked Content

Handles large documents by processing chunks efficiently.
Generates ChatGPT-level questions about key concepts.

Model: microsoft/Phi-3-mini-4k-instruct (3.8GB)
Quality: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Excellent (ChatGPT-level)
Cost: $0 - Completely free, runs locally, no API limits
Mode: Optimized for processing large document chunks

Features:
- Generates questions about KEY CONCEPTS (not random words)
- Processes content chunks from large documents
- Quality filters reject trivial questions
- Full document coverage via chunking
"""

from flask import Flask, request, jsonify
from flask_cors import CORS
import json
import logging
import re
from typing import Dict, List
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

app = Flask(__name__)
CORS(app)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class Phi3QuizGenerator:
    def __init__(self):
        """Initialize Phi-3-Mini for quiz generation"""
        logger.info("=" * 70)
        logger.info("üöÄ Initializing Phi-3-Mini Quiz Generator (Chunked Mode)...")
        logger.info("=" * 70)
        logger.info("üíª Hardware: Intel i7-13700H, 16GB RAM")
        logger.info("üéØ Quality: ChatGPT-level educational questions")
        logger.info("üí∞ Cost: $0 (Free forever, no API limits)")
        logger.info("üì¶ Mode: Optimized for chunked large documents")
        logger.info("=" * 70)
        
        try:
            model_name = "microsoft/Phi-3-mini-4k-instruct"
            logger.info(f"üì• Loading model: {model_name}")
            logger.info("‚è≥ First time: Downloads ~3.8GB (5-8 minutes)")
            logger.info("‚è≥ Subsequent starts: 10-15 seconds to load")
            
            # Load tokenizer
            logger.info("üî§ Loading tokenizer...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                trust_remote_code=True
            )
            
            # Load model optimized for CPU
            logger.info("üß† Loading Phi-3-Mini model...")
            logger.info("üíæ Memory allocation: ~4GB RAM")
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float32,
                device_map="cpu",
                trust_remote_code=True,
                low_cpu_mem_usage=True
            )
            
            self.model.eval()
            
            logger.info("‚úÖ Phi-3-Mini loaded successfully!")
            logger.info(f"üìä Model size: ~3.8GB")
            logger.info(f"üéì Specialization: Educational question generation")
            logger.info(f"üíæ Context window: 4K tokens (~12K characters per chunk)")
            logger.info(f"‚ö° Expected speed: 15-25 seconds per chunk")
            logger.info("=" * 70)
        except Exception as e:
            logger.error(f"‚ùå Failed to load Phi-3-Mini: {e}")
            logger.error("üí° Make sure you have ~5GB free disk space")
            logger.error("üí° Make sure you have internet for first-time download")
            raise
    def generate_quiz(self, content: str, num_questions: int = 10) -> dict:
        """
        Generate quiz from content chunk.
        This is called multiple times by Laravel for different chunks.
        """
        
        content_length = len(content)
        logger.info(f"üìù Processing chunk: {content_length} characters, {num_questions} questions requested")
        
        # Validate content size
        if content_length < 100:
            logger.warning("Content too short, returning empty quiz")
            return {
                'multiple_choice': [],
                'true_false': []
            }
        
        # Truncate if still too long (safety net)
        if content_length > 12000:
            logger.warning(f"Chunk is {content_length} chars, truncating to 12000")
            content = content[:12000]
        
        # Calculate distribution (30% T/F, 70% MCQ)
        num_tf = min(3, int(num_questions * 0.3))
        num_mcq = num_questions - num_tf
        
        logger.info(f"üìä Distribution: {num_tf} True/False, {num_mcq} Multiple Choice")
        
        quiz_data = {
            'true_false': [],
            'multiple_choice': []
        }
        
        # Generate True/False questions
        logger.info(f"üü¢ Generating {num_tf} True/False questions from chunk...")
        for i in range(num_tf):
            tf_question = self._generate_tf_question(content, i+1)
            if tf_question:
                quiz_data['true_false'].append(tf_question)
        
        # Generate Multiple Choice questions
        logger.info(f"üîµ Generating {num_mcq} Multiple Choice questions from chunk...")
        for i in range(num_mcq):
            mcq_question = self._generate_mcq_question(content, i+1)
            if mcq_question:
                quiz_data['multiple_choice'].append(mcq_question)
        
        total = len(quiz_data['true_false']) + len(quiz_data['multiple_choice'])
        logger.info(f"‚úÖ Chunk processing complete: {total} questions generated")
        
        return quiz_data
    
    def _clean_content(self, content: str, max_length: int = 2000) -> str:
        """Clean and truncate content for better processing"""
        # Remove excessive whitespace
        content = re.sub(r'\s+', ' ', content)
        # Take first portion for context
        return content[:max_length].strip()
    
    def _generate_tf_question(self, content: str, question_num: int) -> dict:
        """Generate a True/False question about KEY CONCEPTS"""
        
        # Enhanced prompt with explicit examples
        prompt = f"""<|system|>You are an expert educational quiz generator specializing in computer science and technical subjects. Generate ONLY questions about MAIN TECHNICAL CONCEPTS, NEVER about page numbers, common words, or document structure.<|end|>
<|user|>
Content:
{content[:2500]}

CRITICAL RULES:
1. Question MUST be about a TECHNICAL CONCEPT: algorithms, data structures, design patterns, frameworks, advantages, purposes
2. NEVER ask about: 'page', 'content', 'chapter', 'hiring', 'firing', 'fixed', common words like 'a', 'the', 'and'
3. Must test TECHNICAL UNDERSTANDING

GOOD EXAMPLES:
- "ExecutorService provides better thread management than manual thread creation - T/F"
- "The Singleton pattern ensures only one instance exists - T/F"
- "Binary search requires a sorted array - T/F"

BAD EXAMPLES (NEVER DO THIS):
- "This content provides information about 'Page' - T/F"
- "The primary focus is 'hiring' - T/F"

Generate ONE true/false question in this format:

Statement: [Technical concept statement]
Answer: [True or False]
Explanation: [Why this is correct]

Generate:<|end|>
<|assistant|>"""

        try:
            inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1024)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=150,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    repetition_penalty=1.1,
                    pad_token_id=self.tokenizer.eos_token_id,
                    use_cache=False
                )
            
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # Extract only the assistant's response
            if "<|assistant|>" in response:
                response = response.split("<|assistant|>")[-1].strip()
            
            # Parse the response
            statement_match = re.search(r'Statement:\s*(.+?)(?=\nAnswer:|\n|$)', response, re.DOTALL | re.IGNORECASE)
            answer_match = re.search(r'Answer:\s*(True|False)', response, re.IGNORECASE)
            explanation_match = re.search(r'Explanation:\s*(.+?)(?=\n\n|$)', response, re.DOTALL | re.IGNORECASE)
            
            if statement_match and answer_match:
                statement = statement_match.group(1).strip()
                
                # Enhanced quality check: reject questions about trivial words
                trivial_patterns = [
                    r'\b(page|pages|content|chapter|section|paragraph|material|information)\b',
                    r'provides (educational )?information about',
                    r'presents detailed analysis',
                    r'written in a language',
                    r'['']+(pools?|thread|executors?|think|hiring|firing|and|or|the|a|an|with)['']',
                    r'\b(the|a|an|and|or|is|are|was|were)\b.+(primarily|characterized)',
                    r'---',
                ]
                
                statement_lower = statement.lower()
                for pattern in trivial_patterns:
                    if re.search(pattern, statement_lower):
                        logger.warning(f"Rejected T/F {question_num}: Contains trivial word pattern: {pattern}")
                        return None
                
                return {
                    'question': statement,
                    'correct_answer': answer_match.group(1).capitalize(),
                    'explanation': explanation_match.group(1).strip() if explanation_match else "Verify this against the content."
                }
        
        except Exception as e:
            logger.warning(f"Failed to parse T/F question {question_num}: {e}")
        
        return None
    
    def _generate_mcq_question(self, content: str, question_num: int) -> dict:
        """Generate a Multiple Choice question about KEY CONCEPTS"""
        
        prompt = f"""<|system|>You are an expert educational quiz generator specializing in computer science and technical subjects. Generate ONLY questions about MAIN TECHNICAL CONCEPTS, NEVER about page numbers, common words, or document structure.<|end|>
<|user|>
Content:
{content[:2500]}

CRITICAL RULES:
1. Question MUST be about a TECHNICAL CONCEPT: algorithms, frameworks, design patterns, advantages, when to use X vs Y
2. NEVER ask "what is the primary focus of 'page/hiring/fixed/a/an'" - these are NOT technical concepts
3. All options must be MEANINGFUL technical differences, not generic phrases like "fundamental concept requiring explanation"
4. Test REAL UNDERSTANDING of how something works, its advantages, or when to use it

GOOD EXAMPLES:
- "What is the main advantage of using ExecutorService over manual thread creation?"
- "Which data structure provides O(1) lookup time?"
- "When should you use a cached thread pool?"

BAD EXAMPLES (NEVER DO THIS):
- "What is the primary focus of 'page'?"
- "What role does 'hiring' play?"
- Options like "A fundamental concept requiring detailed explanation"

Generate ONE MCQ with SPECIFIC technical options:

Question: [Technical question about how/why/when]
A) [First option]
B) [Second option]
C) [Third option]
D) [Fourth option]
Correct: [A, B, C, or D]
Explanation: [Why the correct answer is right]<|end|>
<|assistant|>"""

        try:
            inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1024)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=250,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    repetition_penalty=1.1,
                    pad_token_id=self.tokenizer.eos_token_id,
                    use_cache=False
                )
            
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # Extract only the assistant's response
            if "<|assistant|>" in response:
                response = response.split("<|assistant|>")[-1].strip()
            
            # Parse the response
            question_match = re.search(r'Question:\s*(.+?)(?=\n[A-D]\))', response, re.DOTALL | re.IGNORECASE)
            options = re.findall(r'([A-D])\)\s*(.+?)(?=\n[A-D]\)|\nCorrect:|\n|$)', response, re.DOTALL)
            correct_match = re.search(r'Correct:\s*([A-D])', response, re.IGNORECASE)
            explanation_match = re.search(r'Explanation:\s*(.+?)(?=\n\n|$)', response, re.DOTALL | re.IGNORECASE)
            
            if question_match and len(options) >= 4 and correct_match:
                question_text = question_match.group(1).strip()
                
                # Enhanced quality check: reject questions about trivial words or meta-concepts
                trivial_patterns = [
                    r"(primary focus|primarily characterized|primary significance|can be understood as)",
                    r"(role does|refers to|role.+play)",
                    r"['']+(page|pages|hiring|firing|fixed|instead|like|think|and|or|the|a|an|with|thread|pools?)['']",
                    r"\b(page|pages|content|chapter|section|material|information)\b.+(focus|characterized|significance)",
                    r"fundamental concept requiring detailed explanation",
                    r"minor detail mentioned briefly",
                    r"example used for illustration",
                    r"secondary supporting detail",
                    r"methodological approach",
                    r"future consideration",
                    r"---",
                ]
                
                question_lower = question_text.lower()
                for pattern in trivial_patterns:
                    if re.search(pattern, question_lower):
                        logger.warning(f"Rejected MCQ {question_num}: Contains trivial pattern: {pattern}")
                        return None
                
                formatted_options = [f"{opt[0]}) {opt[1].strip()}" for opt in options[:4]]
                correct_letter = correct_match.group(1).upper()
                
                return {
                    'question': question_text,
                    'options': formatted_options,
                    'correct_answer': correct_letter,
                    'explanation': explanation_match.group(1).strip() if explanation_match else "Review the content for reasoning."
                }
        
        except Exception as e:
            logger.warning(f"Failed to parse MCQ {question_num}: {e}")
        
        return None

# Initialize generator
logger.info("üöÄ Starting Phi-3-Mini Quiz Generator initialization...")
quiz_generator = Phi3QuizGenerator()

@app.route('/', methods=['GET'])
def home():
    """Service info"""
    return jsonify({
        'service': 'Phi-3-Mini Quiz Generation Service',
        'model': 'microsoft/Phi-3-mini-4k-instruct',
        'version': '1.0.0',
        'quality': 'ChatGPT-level (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê)',
        'cost': '$0 (Free forever)',
        'optimized_for': 'Intel i7-13700H, 16GB RAM',
        'endpoints': {
            '/health': 'Health check',
            '/generate-quiz': 'POST - Generate quiz from content'
        }
    })

@app.route('/health', methods=['GET'])
def health():
    """Health check"""
    return jsonify({
        'status': 'healthy',
        'service': 'Phi-3-Mini Quiz Generator (Chunked Mode)',
        'model': 'microsoft/Phi-3-mini-4k-instruct',
        'cost': '$0 (Free - runs locally)',
        'mode': 'optimized_for_chunks',
        'max_chunk_size': '12K characters'
    })

@app.route('/generate-quiz', methods=['POST'])
def generate_quiz():
    """Generate quiz from content chunk"""
    try:
        data = request.json
        content = data.get('content', '')
        num_questions = data.get('num_questions', 10)
        
        if not content or len(content.strip()) < 100:
            return jsonify({
                'success': False,
                'error': 'Content must be at least 100 characters'
            }), 400
        
        # Generate quiz from this chunk
        quiz_data = quiz_generator.generate_quiz(content, num_questions)
        
        return jsonify({
            'success': True,
            'quiz': quiz_data
        })
        
    except Exception as e:
        logger.error(f"‚ùå Error generating quiz: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

if __name__ == '__main__':
    print("\n" + "=" * 80)
    print("üöÄ Phi-3-Mini Quiz Generation Service")
    print("=" * 80)
    print("üíª Optimized for: Intel i7-13700H, 16GB RAM")
    print("üí∞ Cost: $0 (Free forever, no API limits)")
    print("üì¶ Model: microsoft/Phi-3-mini-4k-instruct (3.8GB)")
    print("üéØ Quality: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê ChatGPT-level questions")
    print("‚ö° Speed: 15-25 seconds per quiz on your laptop")
    print("üìç URL: http://localhost:5002")
    print("‚ö†Ô∏è  First run: Downloads ~3.8GB model (5-8 minutes)")
    print("=" * 80)
    print("\nüéì Generates questions about KEY CONCEPTS, not random words!")
    print("‚úÖ Example: 'What is the main advantage of ExecutorService?'")
    print("‚ùå No more: 'What does \\'and\\' refer to?'\n")
    print("=" * 80 + "\n")
    
    app.run(host='0.0.0.0', port=5002, debug=False)
